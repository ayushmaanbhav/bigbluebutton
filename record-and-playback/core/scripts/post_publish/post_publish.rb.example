#!/usr/bin/ruby
# encoding: UTF-8

#
# BigBlueButton open source conferencing system - http://www.bigbluebutton.org/
#
# Copyright (c) 2012 BigBlueButton Inc. and by respective authors (see below).
#
# This program is free software; you can redistribute it and/or modify it under
# the terms of the GNU Lesser General Public License as published by the Free
# Software Foundation; either version 3.0 of the License, or (at your option)
# any later version.
#
# BigBlueButton is distributed in the hope that it will be useful, but WITHOUT
# ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or FITNESS
# FOR A PARTICULAR PURPOSE. See the GNU Lesser General Public License for more
# details.
#
# You should have received a copy of the GNU Lesser General Public License along
# with BigBlueButton; if not, see <http://www.gnu.org/licenses/>.
#

require "trollop"
require 'rubygems'
require 'aws-sdk-s3'
require File.expand_path('../../../lib/recordandplayback', __FILE__)

class S3FolderUpload
  attr_reader :folder_path, :total_files, :s3_bucket, :recordingId, :logger
  attr_accessor :files

  # Initialize the upload class
  #
  # folder_path - path to the folder that you want to upload
  # bucket - The bucket you want to upload to
  # aws_key - Your key generated by AWS defaults to the environemt setting AWS_KEY_ID
  # aws_secret - The secret generated by AWS
  # recordingId - recordingIda
  #
  # Examples
  #   => uploader = S3FolderUpload.new("some_route/test_folder", 'your_bucket_name')
  #
  def initialize(recordingId, folder_path, bucket, aws_key = ENV['AWS_KEY_ID'], aws_secret = ENV['AWS_SECRET'] ,logger)
    Aws.config.update({
      #region: ENV['AWS_REGION'],
      region: 'ap-south-1',
      #credentials: Aws::Credentials.new(ENV['AWS_KEY_ID'], ENV['AWS_SECRET'])
      credentials: Aws::Credentials.new('gibberish', 'gibberish')
    })

    @folder_path       = folder_path
    @files             = Dir.glob("#{folder_path}/**/*")
    @total_files       = files.length
    @connection        = Aws::S3::Resource.new
    @s3_bucket         = @connection.bucket(bucket)
    @recordingId       = recordingId
    @logger            = logger
  end

  # public: Upload files from the folder to S3
  #
  # thread_count - How many threads you want to use (defaults to 5)
  # simulate - Don't perform upload, just simulate it (default: false)
  # verbose - Verbose info (default: false)
  #
  # Examples
  #   => uploader.upload!(20)
  #     true
  #   => uploader.upload!
  #     true
  #
  # Returns true when finished the process
  def upload!(thread_count = 5, simulate = false, verbose = true)
    file_number = 0
#    mutex       = Mutex.new
    threads     = []

    logger.info("Total files: #{total_files}... uploading (folder #{folder_path}, recording #{recordingId})")

    until files.empty?
      file = files.pop rescue nil
      next unless file

      # Define destination path
      path = "#{recordingId}/#{file.sub(/^#{folder_path}\//, '')}"

      logger.info("[#{file_number}/#{total_files}] uploading...") if verbose

      data = File.open(file)

      unless File.directory?(data) || simulate
        obj = s3_bucket.object(path)
        obj.put({ acl: "public-read", body: data })
      end

      data.close
      file_number = file_number + 1
    end
#    thread_count.times do |i|
#      threads[i] = Thread.new {
#        until files.empty?
#          mutex.synchronize do
#            file_number += 1
#            Thread.current["file_number"] = file_number
#          end
#          file = files.pop rescue nil
#          next unless file
#
#          # Define destination path
#          path = recordingId + "/" + file.sub(/^#{folder_path}\//, '')
#
#          logger.info("[#{Thread.current["file_number"]}/#{total_files}] uploading...") if verbose
#
#          data = File.open(file)
#
#          unless File.directory?(data) || simulate
#            obj = s3_bucket.object(path)
#            obj.put({ acl: "public-read", body: data })
#          end
#
#          data.close
#        end
#      }
#    end
#    threads.each { |t| t.join }
  end
end

opts = Trollop::options do
  opt :meeting_id, "Meeting id to archive", :type => String
end
meeting_id = opts[:meeting_id]

logger = Logger.new("/var/log/bigbluebutton/post_publish.log", 'weekly' )
logger.level = Logger::INFO
BigBlueButton.logger = logger

BigBlueButton.logger.info("Recording upload to S3 for [#{meeting_id}] starts")

begin
  published_files = "/var/bigbluebutton/published/presentation/#{meeting_id}"
  meeting_metadata = BigBlueButton::Events.get_meeting_metadata("/var/bigbluebutton/recording/raw/#{meeting_id}/events.xml")
  BigBlueButton.logger.info("Recording upload to S3 for [#{meeting_id}] :: meeting_metadata : #{meeting_metadata}, recordingId: #{meeting_metadata["recordingid"]}")
  uploader = S3FolderUpload.new(meeting_metadata["recordingid"], published_files, 'prod-enlite-recording', ENV['AWS_KEY_ID'], ENV['AWS_SECRET'], BigBlueButton.logger)
  uploader.upload!
rescue StandardError => e
  BigBlueButton.logger.info("Recording upload yo S3 for [#{meeting_id}] errorred [#{e}, #{e.backtrace}]")
  raise e
end

BigBlueButton.logger.info("Recording upload yo S3 for [#{meeting_id}] ends")

exit 0
